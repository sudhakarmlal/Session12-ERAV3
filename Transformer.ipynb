{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krwhATwdjQkD",
        "outputId": "30ffee17-2c90-4054-f615-f9c90073630e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HArzCgoTkJnw",
        "outputId": "cd78e859-45b4-4df8-ff35-040373124635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvcCF742kNcl",
        "outputId": "86f55c1f-0cc6-4407-ee54-7a51cf75e17c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd transformer_input"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RswX6eytkS4a",
        "outputId": "e02efda8-685d-4c0c-b680-d8d87248e7df"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'transformer_input'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORkjKujGk-1s",
        "outputId": "fd6ad9d7-90e4-4681-de97-ac3a4fffc6a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2024.12.14)\n",
            "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ax6v-M8lPJo",
        "outputId": "86f417ed-8be2-4057-e55e-20bde95ed8b5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34m'Colab Notebooks'\u001b[0m/     'model (9).h5'\n",
            " DNN02.ipynb            model.h5\n",
            " \u001b[01;34mEVA4Phase2\u001b[0m/            model_weights_img_aug_120_127_op.h5\n",
            "'Getting started.pdf'   model_weights_img_aug_30_60.h5\n",
            " \u001b[01;34mIMDB\u001b[0m/                  model_weights_img_aug_60_90.h5\n",
            "'model (1).h5'          model_weights_img_aug_90_120.h5\n",
            "'model (2).h5'          model_weights_img_aug_90_120_op.h5\n",
            "'model (3).h5'          model_weights_img_aug.h5\n",
            "'model (4).h5'          model_weights_img_aug_image_man_0_30.h5\n",
            "'model (5).h5'          \u001b[01;34mtransformer_input\u001b[0m/\n",
            "'model (6).h5'          Untitled\n",
            "'model (7).h5'          Untitled0.ipynb\n",
            "'model (8).h5'          Untitled1.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solving for residual std scaling issue\n",
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        self.register_buffer(\n",
        "            \"bias\",\n",
        "            torch.tril(torch.ones(config.block_size, config.block_size)).view(\n",
        "                1, 1, config.block_size, config.block_size\n",
        "            ),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = (\n",
        "            x.size()\n",
        "        )  # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(\n",
        "            1, 2\n",
        "        )  # (B, nh, T, hs)\n",
        "\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "\n",
        "        y = (\n",
        "            y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "        )  # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu = nn.GELU(approximate=\"tanh\")\n",
        "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024  # Reduced from 1024 to save memory\n",
        "    vocab_size: int = 50257\n",
        "    n_layer: int = 12  # Reduced from 12\n",
        "    n_head: int = 12  # Reduced from 12\n",
        "    n_embd: int = 768  # Reduced from 768\n",
        "    dropout: float = 0.1  # Added dropout for regularization\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(\n",
        "            dict(\n",
        "                wte=nn.Embedding(config.vocab_size, config.n_embd),\n",
        "                wpe=nn.Embedding(config.block_size, config.n_embd),\n",
        "                h=nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "                ln_f=nn.LayerNorm(config.n_embd),\n",
        "            )\n",
        "        )\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # weight initialization\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, \"NANGPT_SCALE_INIT\"):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert (\n",
        "            T <= self.config.block_size\n",
        "        ), f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=idx.device)  # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos)  # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)  # (B, T, vocab_size)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, model_type):\n",
        "        \"\"\"Loads pretrained GPT-2 model weights from huggingface\"\"\"\n",
        "        assert model_type in {\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"}\n",
        "        from transformers import GPT2LMHeadModel\n",
        "\n",
        "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "        # n_layer, n_head and n_embd are determined from model_type\n",
        "        config_args = {\n",
        "            \"gpt2\": dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "            \"gpt2-medium\": dict(n_layer=24, n_head=16, n_embd=1024),  # 350M params\n",
        "            \"gpt2-large\": dict(n_layer=36, n_head=20, n_embd=1280),  # 774M params\n",
        "            \"gpt2-xl\": dict(n_layer=48, n_head=25, n_embd=1600),  # 1558M params\n",
        "        }[model_type]\n",
        "        config_args[\"vocab_size\"] = 50257  # always 50257 for GPT model checkpoints\n",
        "        config_args[\"block_size\"] = 1024  # always 1024 for GPT model checkpoints\n",
        "        # create a from-scratch initialized minGPT model\n",
        "        config = GPTConfig(**config_args)\n",
        "        model = GPT(config)\n",
        "        sd = model.state_dict()\n",
        "        sd_keys = sd.keys()\n",
        "        sd_keys = [\n",
        "            k for k in sd_keys if not k.endswith(\".attn.bias\")\n",
        "        ]  # discard this mask / buffer, not a param\n",
        "\n",
        "        # init a huggingface/transformers model\n",
        "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "        sd_hf = model_hf.state_dict()\n",
        "\n",
        "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "        sd_keys_hf = sd_hf.keys()\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.masked_bias\")\n",
        "        ]  # ignore these, just a buffer\n",
        "        sd_keys_hf = [\n",
        "            k for k in sd_keys_hf if not k.endswith(\".attn.bias\")\n",
        "        ]  # same, just the mask (buffer)\n",
        "        transposed = [\n",
        "            \"attn.c_attn.weight\",\n",
        "            \"attn.c_proj.weight\",\n",
        "            \"mlp.c_fc.weight\",\n",
        "            \"mlp.c_proj.weight\",\n",
        "        ]\n",
        "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "        # this means that we have to transpose these weights when we import them\n",
        "        assert len(sd_keys_hf) == len(\n",
        "            sd_keys\n",
        "        ), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "        for k in sd_keys_hf:\n",
        "            if any(k.endswith(w) for w in transposed):\n",
        "                # special treatment for the Conv1D weights we need to transpose\n",
        "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k].t())\n",
        "            else:\n",
        "                # vanilla copy over the other parameters\n",
        "                assert sd_hf[k].shape == sd[k].shape\n",
        "                with torch.no_grad():\n",
        "                    sd[k].copy_(sd_hf[k])\n",
        "\n",
        "        return model\n",
        "\n",
        "\n",
        "# model = GPT.from_pretrained('gpt2')\n",
        "\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# SEED\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "# STOP\n",
        "num_return_sequences = 5\n",
        "max_length = 30\n",
        "\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "class DataLoaderLite:\n",
        "    def __init__(self, B, T):\n",
        "        self.B = B\n",
        "        self.T = T\n",
        "\n",
        "        # at init load tokens from disk and store them in memory\n",
        "        with open(\"/content/drive/MyDrive/transformer_input/input.txt\", \"r\") as f:\n",
        "            text = f.read()\n",
        "        enc = tiktoken.get_encoding(\"gpt2\")\n",
        "        tokens = enc.encode(text)\n",
        "        self.tokens = torch.tensor(tokens)\n",
        "        print(f\"loaded {len(self.tokens)} tokens\")\n",
        "        print(f\"1 epoch = {len(self.tokens) // (B * T)} batches\")\n",
        "\n",
        "        # state\n",
        "        self.current_position = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        B, T = self.B, self.T\n",
        "        buf = self.tokens[self.current_position : self.current_position + B * T + 1]\n",
        "        x = (buf[:-1]).view(B, T)  # inputs\n",
        "        y = (buf[1:]).view(B, T)  # targets\n",
        "        # advance the position in the tensor\n",
        "        self.current_position += B * T\n",
        "        # if loading the next batch would be out of bounds, reset\n",
        "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
        "            self.current_position = 0\n",
        "        return x, y\n",
        "\n",
        "\n",
        "model = GPT(GPTConfig())\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "# Print model size\n",
        "def count_parameters(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    param_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (\n",
        "        1024 * 1024\n",
        "    )  # Size in MB\n",
        "\n",
        "    print(f\"Total Parameters: {total_params:,}\")\n",
        "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
        "    print(f\"Model Size: {param_size:.2f} MB\")\n",
        "\n",
        "    # Print size by layer type\n",
        "    layer_sizes = {}\n",
        "    for name, p in model.named_parameters():\n",
        "        layer_type = name.split(\".\")[0]\n",
        "        size = p.numel() * p.element_size() / (1024 * 1024)  # Size in MB\n",
        "        layer_sizes[layer_type] = layer_sizes.get(layer_type, 0) + size\n",
        "\n",
        "    print(\"\\nSize by component:\")\n",
        "    for layer_type, size in layer_sizes.items():\n",
        "        print(f\"{layer_type}: {size:.2f} MB\")\n",
        "\n",
        "\n",
        "count_parameters(model)\n",
        "\n",
        "# Increase batch size and reduce sequence length\n",
        "train_loader = DataLoaderLite(B=16, T=128)\n",
        "\n",
        "# Calculate total steps\n",
        "total_steps = 20000  # Increased from 10000\n",
        "print(f\"Training for {total_steps} steps\")\n",
        "\n",
        "# Initialize optimizer with better parameters\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(), lr=2e-4, weight_decay=0.1, betas=(0.9, 0.95), eps=1e-8\n",
        ")\n",
        "\n",
        "# Use OneCycleLR scheduler with better parameters\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=2e-4,\n",
        "    total_steps=total_steps,\n",
        "    pct_start=0.1,\n",
        "    anneal_strategy=\"cos\",\n",
        "    cycle_momentum=False,\n",
        "    div_factor=10.0,\n",
        "    final_div_factor=50.0,\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "best_loss = float(\"inf\")\n",
        "step = 0\n",
        "losses = []  # Keep track of losses for monitoring\n",
        "last_time = time.time()\n",
        "interval = 100  # Print every 10 steps\n",
        "moving_avg_size = 100  # Window size for moving average\n",
        "\n",
        "while step < total_steps:\n",
        "    x, y = train_loader.next_batch()\n",
        "    x, y = x.to(device), y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, loss = model(x, y)\n",
        "    loss.backward()\n",
        "\n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)  # Reduced from 1.0\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "    # Update best loss\n",
        "    if loss.item() < best_loss:\n",
        "        best_loss = loss.item()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # Calculate moving average loss\n",
        "    avg_loss = sum(losses[-moving_avg_size:]) / min(len(losses), moving_avg_size)\n",
        "\n",
        "    # Print progress\n",
        "    if step % interval == 0:\n",
        "        current_time = time.time()\n",
        "        time_per_batch = (current_time - last_time) / interval if step > 0 else 0\n",
        "        last_time = current_time\n",
        "\n",
        "        print(\n",
        "            f\"step {step}, \"\n",
        "            f\"loss: {loss.item():.4f}, \"\n",
        "            f\"avg_loss: {avg_loss:.4f}, \"\n",
        "            f\"best_loss: {best_loss:.4f}, \"\n",
        "            f\"lr: {scheduler.get_last_lr()[0]:.2e}, \"\n",
        "            f\"time/batch: {time_per_batch:.3f}s\"\n",
        "        )\n",
        "\n",
        "    # Check if moving average loss is below threshold\n",
        "    if avg_loss < 0.099999:\n",
        "        print(f\"\\nReached target average loss of {avg_loss:.6f} at step {step}\")\n",
        "        break\n",
        "\n",
        "    step += 1\n",
        "\n",
        "print(f\"Final loss: {loss.item():.6f}\")\n",
        "print(f\"Best loss: {best_loss:.6f}\")\n",
        "print(f\"Final average loss: {avg_loss:.6f}\")\n",
        "\n",
        "# Save the trained model with compression\n",
        "save_path = \"trained_model.pt\"\n",
        "torch.save(\n",
        "    {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"config\": model.config,\n",
        "    },\n",
        "    save_path,\n",
        ")\n",
        "print(f\"Model saved to {save_path}\")\n",
        "\n",
        "# Generation code\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "prompt = \"We are accounted poor citizens, the\"\n",
        "tokens = enc.encode(prompt)\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POMNly-KkVm1",
        "outputId": "a065b91b-53bc-4a4a-b134-363ac1e456da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n",
            "Total Parameters: 124,439,808\n",
            "Trainable Parameters: 124,439,808\n",
            "Model Size: 474.70 MB\n",
            "\n",
            "Size by component:\n",
            "transformer: 474.70 MB\n",
            "loaded 338025 tokens\n",
            "1 epoch = 165 batches\n",
            "Training for 20000 steps\n",
            "step 0, loss: 10.9502, avg_loss: 10.9502, best_loss: 10.9502, lr: 2.00e-05, time/batch: 0.000s\n",
            "step 100, loss: 7.3847, avg_loss: 8.3567, best_loss: 7.3148, lr: 2.11e-05, time/batch: 0.493s\n",
            "step 200, loss: 6.1754, avg_loss: 6.8560, best_loss: 6.1491, lr: 2.45e-05, time/batch: 0.525s\n",
            "step 300, loss: 5.9571, avg_loss: 6.2103, best_loss: 5.6249, lr: 2.99e-05, time/batch: 0.518s\n",
            "step 400, loss: 5.5309, avg_loss: 5.8262, best_loss: 5.3845, lr: 3.73e-05, time/batch: 0.522s\n",
            "step 500, loss: 5.6939, avg_loss: 5.6099, best_loss: 4.8133, lr: 4.65e-05, time/batch: 0.523s\n",
            "step 600, loss: 5.5644, avg_loss: 5.3130, best_loss: 4.7059, lr: 5.72e-05, time/batch: 0.525s\n",
            "step 700, loss: 5.1696, avg_loss: 5.1070, best_loss: 4.2967, lr: 6.93e-05, time/batch: 0.525s\n",
            "step 800, loss: 4.3263, avg_loss: 4.9632, best_loss: 4.2546, lr: 8.24e-05, time/batch: 0.524s\n",
            "step 900, loss: 5.0162, avg_loss: 4.7396, best_loss: 3.9474, lr: 9.61e-05, time/batch: 0.524s\n",
            "step 1000, loss: 4.3281, avg_loss: 4.6276, best_loss: 3.6839, lr: 1.10e-04, time/batch: 0.524s\n",
            "step 1100, loss: 4.6886, avg_loss: 4.4523, best_loss: 3.6839, lr: 1.24e-04, time/batch: 0.524s\n",
            "step 1200, loss: 4.3452, avg_loss: 4.3065, best_loss: 3.4548, lr: 1.38e-04, time/batch: 0.525s\n",
            "step 1300, loss: 4.2730, avg_loss: 4.2795, best_loss: 3.4548, lr: 1.51e-04, time/batch: 0.526s\n",
            "step 1400, loss: 3.8989, avg_loss: 4.0641, best_loss: 3.2578, lr: 1.63e-04, time/batch: 0.525s\n",
            "step 1500, loss: 3.6982, avg_loss: 4.0088, best_loss: 3.0768, lr: 1.74e-04, time/batch: 0.524s\n",
            "step 1600, loss: 4.2477, avg_loss: 3.8924, best_loss: 3.0768, lr: 1.83e-04, time/batch: 0.524s\n",
            "step 1700, loss: 3.7943, avg_loss: 3.7073, best_loss: 2.8866, lr: 1.90e-04, time/batch: 0.524s\n",
            "step 1800, loss: 3.4301, avg_loss: 3.6525, best_loss: 2.8866, lr: 1.96e-04, time/batch: 0.524s\n",
            "step 1900, loss: 3.3754, avg_loss: 3.4688, best_loss: 2.7194, lr: 1.99e-04, time/batch: 0.524s\n",
            "step 2000, loss: 3.4018, avg_loss: 3.3635, best_loss: 2.5421, lr: 2.00e-04, time/batch: 0.524s\n",
            "step 2100, loss: 3.4427, avg_loss: 3.2114, best_loss: 2.5421, lr: 2.00e-04, time/batch: 0.524s\n",
            "step 2200, loss: 3.2218, avg_loss: 3.0127, best_loss: 2.3175, lr: 2.00e-04, time/batch: 0.525s\n",
            "step 2300, loss: 2.4893, avg_loss: 2.8841, best_loss: 2.3175, lr: 2.00e-04, time/batch: 0.525s\n",
            "step 2400, loss: 2.5623, avg_loss: 2.7000, best_loss: 2.0940, lr: 2.00e-04, time/batch: 0.525s\n",
            "step 2500, loss: 2.1603, avg_loss: 2.5539, best_loss: 1.8928, lr: 2.00e-04, time/batch: 0.525s\n",
            "step 2600, loss: 2.5898, avg_loss: 2.3848, best_loss: 1.8827, lr: 1.99e-04, time/batch: 0.525s\n",
            "step 2700, loss: 2.0253, avg_loss: 2.1897, best_loss: 1.6191, lr: 1.99e-04, time/batch: 0.525s\n",
            "step 2800, loss: 2.2851, avg_loss: 2.0429, best_loss: 1.4060, lr: 1.99e-04, time/batch: 0.525s\n",
            "step 2900, loss: 1.8762, avg_loss: 1.8798, best_loss: 1.4025, lr: 1.99e-04, time/batch: 0.524s\n",
            "step 3000, loss: 1.6286, avg_loss: 1.7625, best_loss: 1.1947, lr: 1.98e-04, time/batch: 0.525s\n",
            "step 3100, loss: 1.4322, avg_loss: 1.5737, best_loss: 1.1822, lr: 1.98e-04, time/batch: 0.526s\n",
            "step 3200, loss: 1.4968, avg_loss: 1.4329, best_loss: 1.0328, lr: 1.98e-04, time/batch: 0.525s\n",
            "step 3300, loss: 1.6582, avg_loss: 1.3049, best_loss: 0.8372, lr: 1.97e-04, time/batch: 0.524s\n",
            "step 3400, loss: 0.8905, avg_loss: 1.1427, best_loss: 0.8372, lr: 1.97e-04, time/batch: 0.525s\n",
            "step 3500, loss: 0.7788, avg_loss: 1.0810, best_loss: 0.7237, lr: 1.97e-04, time/batch: 0.526s\n",
            "step 3600, loss: 0.9048, avg_loss: 0.9375, best_loss: 0.6515, lr: 1.96e-04, time/batch: 0.526s\n",
            "step 3700, loss: 0.6757, avg_loss: 0.8240, best_loss: 0.5338, lr: 1.96e-04, time/batch: 0.525s\n",
            "step 3800, loss: 1.0375, avg_loss: 0.7583, best_loss: 0.4709, lr: 1.95e-04, time/batch: 0.525s\n",
            "step 3900, loss: 0.7747, avg_loss: 0.6208, best_loss: 0.4044, lr: 1.95e-04, time/batch: 0.525s\n",
            "step 4000, loss: 0.5498, avg_loss: 0.6080, best_loss: 0.3479, lr: 1.94e-04, time/batch: 0.525s\n",
            "step 4100, loss: 0.4709, avg_loss: 0.5106, best_loss: 0.3477, lr: 1.93e-04, time/batch: 0.525s\n",
            "step 4200, loss: 0.4131, avg_loss: 0.4460, best_loss: 0.2473, lr: 1.93e-04, time/batch: 0.526s\n",
            "step 4300, loss: 0.3535, avg_loss: 0.4251, best_loss: 0.2456, lr: 1.92e-04, time/batch: 0.525s\n",
            "step 4400, loss: 0.3723, avg_loss: 0.3336, best_loss: 0.2063, lr: 1.91e-04, time/batch: 0.525s\n",
            "step 4500, loss: 0.2995, avg_loss: 0.3353, best_loss: 0.2046, lr: 1.91e-04, time/batch: 0.526s\n",
            "step 4600, loss: 0.3544, avg_loss: 0.3004, best_loss: 0.1964, lr: 1.90e-04, time/batch: 0.526s\n",
            "step 4700, loss: 0.2422, avg_loss: 0.2600, best_loss: 0.1817, lr: 1.89e-04, time/batch: 0.526s\n",
            "step 4800, loss: 0.2390, avg_loss: 0.2582, best_loss: 0.1649, lr: 1.88e-04, time/batch: 0.527s\n",
            "step 4900, loss: 0.2691, avg_loss: 0.2139, best_loss: 0.1429, lr: 1.87e-04, time/batch: 0.527s\n",
            "step 5000, loss: 0.1830, avg_loss: 0.2160, best_loss: 0.1429, lr: 1.87e-04, time/batch: 0.526s\n",
            "step 5100, loss: 0.1959, avg_loss: 0.2007, best_loss: 0.1330, lr: 1.86e-04, time/batch: 0.526s\n",
            "step 5200, loss: 0.2188, avg_loss: 0.1791, best_loss: 0.1283, lr: 1.85e-04, time/batch: 0.526s\n",
            "step 5300, loss: 0.1573, avg_loss: 0.1774, best_loss: 0.1165, lr: 1.84e-04, time/batch: 0.526s\n",
            "step 5400, loss: 0.1770, avg_loss: 0.1656, best_loss: 0.1120, lr: 1.83e-04, time/batch: 0.526s\n",
            "step 5500, loss: 0.1777, avg_loss: 0.1617, best_loss: 0.1120, lr: 1.82e-04, time/batch: 0.526s\n",
            "step 5600, loss: 0.1426, avg_loss: 0.1575, best_loss: 0.1120, lr: 1.81e-04, time/batch: 0.526s\n",
            "step 5700, loss: 0.1262, avg_loss: 0.1426, best_loss: 0.1105, lr: 1.80e-04, time/batch: 0.526s\n",
            "step 5800, loss: 0.1084, avg_loss: 0.1401, best_loss: 0.0899, lr: 1.79e-04, time/batch: 0.526s\n",
            "step 5900, loss: 0.1477, avg_loss: 0.1333, best_loss: 0.0855, lr: 1.78e-04, time/batch: 0.526s\n",
            "step 6000, loss: 0.1111, avg_loss: 0.1287, best_loss: 0.0855, lr: 1.77e-04, time/batch: 0.526s\n",
            "step 6100, loss: 0.1590, avg_loss: 0.1276, best_loss: 0.0855, lr: 1.75e-04, time/batch: 0.526s\n",
            "step 6200, loss: 0.1100, avg_loss: 0.1192, best_loss: 0.0855, lr: 1.74e-04, time/batch: 0.526s\n",
            "step 6300, loss: 0.1254, avg_loss: 0.1192, best_loss: 0.0853, lr: 1.73e-04, time/batch: 0.526s\n",
            "step 6400, loss: 0.1046, avg_loss: 0.1132, best_loss: 0.0853, lr: 1.72e-04, time/batch: 0.526s\n",
            "step 6500, loss: 0.1156, avg_loss: 0.1087, best_loss: 0.0810, lr: 1.71e-04, time/batch: 0.526s\n",
            "step 6600, loss: 0.1478, avg_loss: 0.1064, best_loss: 0.0743, lr: 1.70e-04, time/batch: 0.526s\n",
            "step 6700, loss: 0.0738, avg_loss: 0.1042, best_loss: 0.0673, lr: 1.68e-04, time/batch: 0.526s\n",
            "step 6800, loss: 0.1058, avg_loss: 0.1035, best_loss: 0.0672, lr: 1.67e-04, time/batch: 0.525s\n",
            "\n",
            "Reached target average loss of 0.099990 at step 6820\n",
            "Final loss: 0.086936\n",
            "Best loss: 0.067244\n",
            "Final average loss: 0.099990\n",
            "Model saved to trained_model.pt\n"
          ]
        }
      ]
    }
  ]
}